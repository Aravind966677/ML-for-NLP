{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpVz3aMrNluA",
        "outputId": "5b5fe75c-316a-43c4-ca74-99cefe45e5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import Word2Vec as GensimWord2Vec\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker_tab')  # Updated chunker name\n",
        "nltk.download('words')\n",
        "nltk.download('tagsets')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is a fascinating field. It enables machines to understand human language.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Sentences:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YtFg2sHQL1D",
        "outputId": "aa736c5e-9645-4fa8-fda8-d3733e568899"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', '.', 'It', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n",
            "Sentences: ['Natural Language Processing is a fascinating field.', 'It enables machines to understand human language.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Tokens (no stopwords):\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmQLC7VyNobH",
        "outputId": "012dbf92-40c1-4e55-80ee-318cb3cb58bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (no stopwords): ['Natural', 'Language', 'Processing', 'fascinating', 'field', '.', 'enables', 'machines', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Stopwords Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Tokens (no stopwords):\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBPVSY2PQdna",
        "outputId": "233f09b2-2675-42d8-f495-b399e8550bda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (no stopwords): ['Natural', 'Language', 'Processing', 'fascinating', 'field', '.', 'enables', 'machines', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4oJd5drQhY-",
        "outputId": "6a68a45e-74c8-4cdf-99a3-e9c67c6351bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens: ['natur', 'languag', 'process', 'fascin', 'field', '.', 'enabl', 'machin', 'understand', 'human', 'languag', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYJHBGT8Qk1u",
        "outputId": "59363560-1a0f-435a-f539-ec3ed040994b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['Natural', 'Language', 'Processing', 'fascinating', 'field', '.', 'enables', 'machine', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New SectionCommon POS Tags\n",
        "Here are some common POS tags used by NLTK:\n",
        "\n",
        "NN: Noun, singular\n",
        "NNS: Noun, plural\n",
        "NNP: Proper noun, singular\n",
        "VB: Verb, base form\n",
        "VBD: Verb, past tense\n",
        "VBG: Verb, gerund/present participle\n",
        "JJ: Adjective\n",
        "RB: Adverb\n",
        "IN: Preposition or subordinating conjunction\n",
        "CC: Coordinating conjunction\n",
        "PRP: Personal pronoun"
      ],
      "metadata": {
        "id": "SPtZ4rmhRTEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gxFXPLlQnp6",
        "outputId": "5cfd5690-0ba0-41dc-e6c9-8c82b7b5288b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('.', '.'), ('It', 'PRP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition (NER)\n",
        "ner = nltk.ne_chunk(pos_tags)\n",
        "print(\"Named Entities:\", ner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIt6IYm6QqAf",
        "outputId": "7111775b-fc91-4a53-ad7d-4234b711f8a7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: (S\n",
            "  Natural/JJ\n",
            "  Language/NNP\n",
            "  Processing/NNP\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  fascinating/JJ\n",
            "  field/NN\n",
            "  ./.\n",
            "  It/PRP\n",
            "  enables/VBZ\n",
            "  machines/NNS\n",
            "  to/TO\n",
            "  understand/VB\n",
            "  human/JJ\n",
            "  language/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding\n",
        "unique_words = set(filtered_tokens)\n",
        "one_hot = {word: np.eye(len(unique_words))[i] for i, word in enumerate(unique_words)}\n",
        "print(\"One-Hot Encodings:\", one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNeo3DTcQuBX",
        "outputId": "9e6cc745-b06a-47f5-f0e7-b8ba93b1e985"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Encodings: {'Language': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Natural': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), '.': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), 'machines': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), 'Processing': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), 'fascinating': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), 'field': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), 'understand': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), 'language': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), 'human': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), 'enables': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words (BoW)\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform([text])\n",
        "print(\"Bag of Words (BoW):\\n\", bow_matrix.toarray())\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFjP_FtSQw8d",
        "outputId": "5b458d60-14a1-4e97-9cf8-d95286ab88fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW):\n",
            " [[1 1 1 1 1 1 2 1 1 1 1 1]]\n",
            "Feature Names: ['enables' 'fascinating' 'field' 'human' 'is' 'it' 'language' 'machines'\n",
            " 'natural' 'processing' 'to' 'understand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n-grams\n",
        "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 2))  # Bigram example\n",
        "ngrams_matrix = vectorizer_ngrams.fit_transform([text])\n",
        "print(\"n-grams (Bigrams):\\n\", ngrams_matrix.toarray())\n",
        "print(\"Feature Names (Bigrams):\", vectorizer_ngrams.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmmBOk5YQ0Rt",
        "outputId": "e8582dd8-4860-4f37-a95e-2fbfc8c38023"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n-grams (Bigrams):\n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "Feature Names (Bigrams): ['enables machines' 'fascinating field' 'field it' 'human language'\n",
            " 'is fascinating' 'it enables' 'language processing' 'machines to'\n",
            " 'natural language' 'processing is' 'to understand' 'understand human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
        "print(\"TF-IDF Feature Names:\", tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APeQBes0Q21W",
        "outputId": "06eb3ff6-66d2-4d3a-8bb5-619b6b945a9a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            " [[0.25819889 0.25819889 0.25819889 0.25819889 0.25819889 0.25819889\n",
            "  0.51639778 0.25819889 0.25819889 0.25819889 0.25819889 0.25819889]]\n",
            "TF-IDF Feature Names: ['enables' 'fascinating' 'field' 'human' 'is' 'it' 'language' 'machines'\n",
            " 'natural' 'processing' 'to' 'understand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings using Word2Vec\n",
        "# Tokenizing sentences for Word2Vec\n",
        "sentence_tokens = [word_tokenize(sent) for sent in sentences]"
      ],
      "metadata": {
        "id": "-uOTNMSWQ5G2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Word2Vec Model - CBOW\n",
        "word2vec_model_cbow = GensimWord2Vec(sentence_tokens, vector_size=100, window=5, min_count=1, sg=0)  # CBOW\n",
        "print(\"Word2Vec CBOW Embedding for 'language':\", word2vec_model_cbow.wv['language'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhoMANISQ5DS",
        "outputId": "c01880fb-ebab-48d6-f40b-c35cf2e54d6f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec CBOW Embedding for 'language': [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Word2Vec Model - Skip-gram\n",
        "word2vec_model_skipgram = GensimWord2Vec(sentence_tokens, vector_size=100, window=5, min_count=1, sg=1)  # Skip-gram\n",
        "print(\"Word2Vec Skip-gram Embedding for 'language':\", word2vec_model_skipgram.wv['language'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVuPYzSHQ-sj",
        "outputId": "16aa6117-b685-4d42-a0a6-4464b86ab283"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Skip-gram Embedding for 'language': [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlFlR0uFRMZu"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}